{"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"Jn9VPwuOm_3M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-uDjgAy5gfqG"},"outputs":[],"source":["!pip install opencv-python-headless\n","!pip install tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SRe-wsP2g86X","outputId":"74b78c96-996f-463e-e938-dbd4815865c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIn3ZkMSxzRf"},"outputs":[],"source":["!unzip \"/content/drive/My Drive/archive.zip\" -d \"/content/drive/My Drive/dataset3\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsG1VjEXyIAN"},"outputs":[],"source":["!df -h"]},{"cell_type":"markdown","source":["https://www.kaggle.com/competitions/deepfake-detection-challenge/data"],"metadata":{"id":"m0wmcwYCnCQz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zqawVXX_spVW"},"outputs":[],"source":["import os\n","import json\n","import cv2\n","import librosa\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, GlobalAveragePooling2D, GlobalAveragePooling1D, Concatenate, GlobalAveragePooling3D, Conv3D, BatchNormalization, MaxPooling3D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.utils.class_weight import compute_class_weight\n","from keras.layers import TimeDistributed, Conv3D, Reshape\n","from tensorflow.keras.applications import MobileNetV2\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V6HZQXwY0JFd","outputId":"5f854d27-27a6-41ec-a3d5-62ca91a8f695"},"outputs":[{"name":"stdout","output_type":"stream","text":["unzip:  cannot find or open /content/drive/My Drive/deepfake-detection-challenge.zip, /content/drive/My Drive/deepfake-detection-challenge.zip.zip or /content/drive/My Drive/deepfake-detection-challenge.zip.ZIP.\n"]}],"source":["!unzip -o \"/content/drive/My Drive/deepfake-detection-challenge.zip\" -d \"/content/drive/My Drive/datasetvd\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"boRMXtAY0vhp"},"outputs":[],"source":["def load_metadata(path):\n","\n","    with open(path, 'r') as file:\n","        return json.load(file)\n","\n","def extract_frames(video_path, num_frames=4):\n","\n","    cap = cv2.VideoCapture(video_path)\n","\n","    frames = []\n","\n","    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    frames_to_capture = np.linspace(0, frame_count - 1, num_frames, dtype=int)\n","\n","    for i in frames_to_capture:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","\n","        success, frame = cap.read()\n","\n","        if success:\n","            frame = cv2.resize(frame, (224, 224))\n","\n","            frame = frame / 255.0\n","\n","            frames.append(frame)\n","        else:\n","            break\n","\n","    cap.release()\n","\n","    while len(frames) < num_frames:\n","        frames.append(np.zeros((224, 224, 3)))\n","\n","    return np.array(frames)\n","\n","def extract_audio_features(video_path, sr=22050, n_mfcc=13):\n","\n","    y, _ = librosa.load(video_path, sr=sr, duration=5.0)\n","\n","    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n","\n","    mfcc = np.mean(mfcc, axis=1)\n","\n","    return mfcc\n","\n","\n","def load_and_process_data(data_dir, metadata):\n","\n","    video_features = []\n","\n","    audio_features = []\n","\n","    labels = []\n","\n","    for filename, info in metadata.items():\n","\n","        video_path = os.path.join(data_dir, filename)\n","\n","        frames = extract_frames(video_path)\n","\n","        mfcc = extract_audio_features(video_path)\n","\n","        video_features.append(frames)\n","\n","        audio_features.append(mfcc)\n","\n","        labels.append(1 if info['label'] == 'FAKE' else 0)\n","\n","    return np.array(video_features), np.array(audio_features), np.array(labels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v49w4QQR5wDA"},"outputs":[],"source":["def build_video_model():\n","\n","    video_input = Input(shape=(4, 224, 224, 3))\n","\n","    x = Conv3D(32, kernel_size=(3, 3, 3), activation='relu')(video_input)\n","\n","    x = BatchNormalization()(x)\n","\n","    x = MaxPooling3D(pool_size=(2, 2, 2))(x)\n","\n","    x = Conv3D(64, kernel_size=(3, 3, 3), activation='relu')(x)\n","\n","    x = BatchNormalization()(x)\n","\n","    x = MaxPooling3D(pool_size=(2, 2, 2))(x)\n","\n","    x = GlobalAveragePooling3D()(x)\n","\n","    x = Dense(256, activation='relu')(x)\n","\n","    x = Dropout(0.5)(x)\n","\n","    x = Dense(128, activation='relu')(x)\n","\n","    x = Dropout(0.5)(x)\n","\n","    return Model(inputs=video_input, outputs=x)\n","\n","def build_audio_model(input_shape=(None, 13)):\n","\n","    audio_input = Input(shape=(13,))\n","\n","    x = Dense(256, activation='relu')(audio_input)\n","\n","    x = Dropout(0.5)(x)\n","\n","    x = Dense(128, activation='relu')(x)\n","\n","    x = Dropout(0.5)(x)\n","\n","    return Model(inputs=audio_input, outputs=x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtra3RpF9u_E"},"outputs":[],"source":["metadata_path = '/content/drive/My Drive/datasetvd/train_sample_videos/metadata.json'\n","\n","data_dir = '/content/drive/My Drive/datasetvd/train_sample_videos'\n","\n","metadata = load_metadata(metadata_path)\n","\n","video_data, audio_data, labels = load_and_process_data(data_dir, metadata)\n","\n","train_val_videos, test_videos, train_val_audios, test_audios, train_val_labels, test_labels = train_test_split(\n","    video_data, audio_data, labels, test_size=0.2, random_state=42)\n","\n","train_videos, val_videos, train_audios, val_audios, train_labels, val_labels = train_test_split(\n","    train_val_videos, train_val_audios, train_val_labels, test_size=0.25, random_state=42)\n","\n","print(\"Number of videos in training set:\", len(train_videos))\n","\n","print(\"Number of videos in validation set:\", len(val_videos))\n","\n","print(\"Number of videos in testing set:\", len(test_videos))\n","\n","print(\"Number of audios in training set:\", len(train_audios))\n","\n","print(\"Number of audios in validation set:\", len(val_audios))\n","\n","print(\"Number of audios in testing set:\", len(test_audios))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsArLunYBwTY"},"outputs":[],"source":["video_model = build_video_model()\n","\n","audio_model = build_audio_model()\n","\n","combined_input = Concatenate()([video_model.output, audio_model.output])\n","\n","x = Dense(64, activation='relu')(combined_input)\n","\n","final_output = Dense(1, activation='sigmoid')(x)\n","\n","model = Model(inputs=[video_model.input, audio_model.input], outputs=final_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LZ6rQs_a9sqs"},"outputs":[],"source":["class_weights = compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(train_labels),\n","    y=train_labels\n",")\n","\n","class_weights_dict = {i : class_weights[i] for i in range(len(class_weights))}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ecBqivH--u6J"},"outputs":[],"source":["model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZlhLyq-eBaJE","outputId":"e4562b80-6cce-40cc-ba21-509fb1a60876"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","60/60 [==============================] - 19s 103ms/step - loss: 1.0381 - accuracy: 0.5042 - val_loss: 0.7068 - val_accuracy: 0.4125\n","Epoch 2/10\n","60/60 [==============================] - 3s 51ms/step - loss: 0.7614 - accuracy: 0.4708 - val_loss: 0.7316 - val_accuracy: 0.2000\n","Epoch 3/10\n","60/60 [==============================] - 3s 50ms/step - loss: 0.7034 - accuracy: 0.4625 - val_loss: 0.6478 - val_accuracy: 0.8250\n","Epoch 4/10\n","60/60 [==============================] - 3s 52ms/step - loss: 0.8929 - accuracy: 0.6625 - val_loss: 0.9494 - val_accuracy: 0.1125\n","Epoch 5/10\n","60/60 [==============================] - 4s 69ms/step - loss: 0.6874 - accuracy: 0.4625 - val_loss: 0.5440 - val_accuracy: 0.8875\n","Epoch 6/10\n","60/60 [==============================] - 3s 50ms/step - loss: 0.6902 - accuracy: 0.5625 - val_loss: 0.6348 - val_accuracy: 0.8750\n","Epoch 7/10\n","60/60 [==============================] - 3s 50ms/step - loss: 0.6810 - accuracy: 0.5833 - val_loss: 0.7010 - val_accuracy: 0.8500\n","Epoch 8/10\n","60/60 [==============================] - 3s 53ms/step - loss: 0.6367 - accuracy: 0.7417 - val_loss: 0.6603 - val_accuracy: 0.8500\n","Epoch 9/10\n","60/60 [==============================] - 3s 54ms/step - loss: 0.6158 - accuracy: 0.7333 - val_loss: 0.9164 - val_accuracy: 0.8375\n","Epoch 10/10\n","60/60 [==============================] - 3s 50ms/step - loss: 0.5929 - accuracy: 0.6667 - val_loss: 0.7519 - val_accuracy: 0.3000\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x7bf8fb34eaa0>"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["model.fit([train_videos, train_audios], train_labels, validation_data=([val_videos, val_audios], val_labels), epochs=10, batch_size=4, class_weight=class_weights_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grYIWtboCOZS","outputId":"1888081f-098c-48bc-8f11-d685c7fb8e7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["3/3 [==============================] - 0s 47ms/step - loss: 0.5267 - accuracy: 0.7875\n","Test Accuracy: 78.75%\n"]}],"source":["test_loss, test_accuracy = model.evaluate([test_videos, test_audios], test_labels)\n","\n","print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBBOqhl0DLRH","outputId":"7480c12b-1639-48a9-d903-31ee061ff9eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["3/3 [==============================] - 0s 49ms/step\n","Confusion Matrix:\n","[[ 2 10]\n"," [ 7 61]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.22      0.17      0.19        12\n","           1       0.86      0.90      0.88        68\n","\n","    accuracy                           0.79        80\n","   macro avg       0.54      0.53      0.53        80\n","weighted avg       0.76      0.79      0.77        80\n","\n"]}],"source":["predictions = model.predict([test_videos, test_audios])\n","\n","predicted_labels = (predictions > 0.5).astype(int)\n","\n","true_labels = test_labels\n","\n","conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","print(\"Confusion Matrix:\")\n","\n","print(conf_matrix)\n","\n","class_report = classification_report(true_labels, predicted_labels)\n","\n","print(\"Classification Report:\")\n","\n","print(class_report)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztwt-7PL95Hm"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}